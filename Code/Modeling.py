# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQIKHgftDGuu-rovHno-zIA_1203EQbt
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive

drive.mount('/content/drive')

full_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/full_data3.csv') # cleaned data from R

full_data.info()

A = 17.625
B = 243.04

# Convert Kelvin to Celsius
full_data['t2m_c'] = full_data['t2m'] - 273.15
full_data['d2m_c'] = full_data['d2m'] - 273.15

# Compute es and ea (unitless)
full_data['es'] = np.exp((A * full_data['t2m_c']) / (full_data['t2m_c'] + B))
full_data['ea'] = np.exp((A * full_data['d2m_c']) / (full_data['d2m_c'] + B))

# Unitless VPD
full_data['vpd'] = full_data['es'] - full_data['ea']

# Drop intermediate columns
full_data.drop(columns=['t2m_c', 'd2m_c', 'es', 'ea'], inplace=True)

full_data.info()

full_data['time'] = pd.to_datetime(full_data['time'])


sample_tday = (
    full_data.groupby('sample')
    .agg({
        'fire_occured': 'first',
        'time': 'max'
    })
    .reset_index()
    .assign(t_day=lambda df: df['time'] + pd.Timedelta(days=1))
)
sample_tday

train_ids = sample_tday[
    (sample_tday['t_day'] >= '2006-01-01') &
    (sample_tday['t_day'] <= '2019-12-31')
]['sample']

val_ids = sample_tday[
    (sample_tday['t_day'] >= '2020-01-01') &
    (sample_tday['t_day'] <= '2020-12-31')
]['sample']

test_ids = sample_tday[
    (sample_tday['t_day'] >= '2021-01-01') &
    (sample_tday['t_day'] <= '2022-12-31')
]['sample']

train_df = full_data[full_data['sample'].isin(train_ids)]
val_df   = full_data[full_data['sample'].isin(val_ids)]
test_df  = full_data[full_data['sample'].isin(test_ids)]

print(sample_tday[sample_tday['sample'].isin(train_ids)]['t_day'].agg(['min', 'max']))
print(sample_tday[sample_tday['sample'].isin(val_ids)]['t_day'].agg(['min', 'max']))
print(sample_tday[sample_tday['sample'].isin(test_ids)]['t_day'].agg(['min', 'max']))

from sklearn.preprocessing import StandardScaler, MinMaxScaler
exclude_cols = ['sample', 'time', 'time_idx', 'fire_occured', 'x', 'y']
exclude_cols += [
    'lc_agriculture',
    'lc_forest',
    'lc_grassland',
    'lc_settlement',
    'lc_shrubland',
    'lc_sparse_vegetation',
    'lc_water_bodies',
    'lc_wetland'
]
feature_cols = [col for col in full_data.columns if col not in exclude_cols]
feature_cols

scaler = StandardScaler()

#scaler = MinMaxScaler()

scaler.fit(train_df[feature_cols])


train_df.loc[:, feature_cols] = scaler.transform(train_df[feature_cols])
val_df.loc[:, feature_cols] = scaler.transform(val_df[feature_cols])
test_df.loc[:, feature_cols] = scaler.transform(test_df[feature_cols])

train_df.groupby('sample').agg({'fire_occured': 'first'}).value_counts()
val_df.groupby('sample').agg({'fire_occured': 'first'}).value_counts()
test_df.groupby('sample').agg({'fire_occured': 'first'}).value_counts() # everything is good!

def reshape_for_lstm(df, feature_cols):

    df_sorted = df.sort_values(['sample', 'time_idx'])

    samples = df_sorted['sample'].unique()
    n_samples = len(samples)
    n_timesteps = df_sorted['time_idx'].nunique()
    n_features = len(feature_cols)


    X = np.empty((n_samples, n_timesteps, n_features), dtype=np.float32)


    for i, sample_id in enumerate(samples):
        sample_data = df_sorted[df_sorted['sample'] == sample_id]

        sample_data = sample_data.sort_values('time_idx')
        X[i, :, :] = sample_data[feature_cols].values

    return X


feature_cols = [col for col in train_df.columns if col not in ['sample', 'time_idx', 'fire_occured', 'time']]
X_train = reshape_for_lstm(train_df, feature_cols)
X_val = reshape_for_lstm(val_df, feature_cols)
X_test = reshape_for_lstm(test_df, feature_cols)

print('Training input shape:', X_train.shape) # Training input shape: (16370, 30, 28)
print('Validation input shape:', X_val.shape) # Validation input shape: (2204, 30, 28)
print('Test input shape:', X_test.shape) # Test input shape: (3967, 30, 28)

y_train = train_df.groupby('sample')['fire_occured'].first().values
y_val = val_df.groupby('sample')['fire_occured'].first().values
y_test = test_df.groupby('sample')['fire_occured'].first().values

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Convert NumPy arrays to torch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2 , dropout = 0.2)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()  # Binary classification

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_hidden = lstm_out[:, -1, :]
        out = self.fc(last_hidden)
        out = self.sigmoid(out)
        return out.squeeze(-1)

n_features = X_train.shape[2]

model = LSTMClassifier(input_dim=n_features)
model = model.to(device)

pip install torchmetrics

criterion = nn.BCELoss()  # Binary cross-entropy
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

!pip install --upgrade torchmetrics

import torchmetrics
from torchmetrics.classification import BinaryAccuracy, BinaryAUROC, BinaryPrecision, BinaryRecall, BinaryF1Score


accuracy_metric = BinaryAccuracy().to(device)
auc_metric = BinaryAUROC().to(device)
precision_metric = BinaryPrecision().to(device)
recall_metric = BinaryRecall().to(device)
f1_metric = BinaryF1Score().to(device)

n_epochs = 10

for epoch in range(n_epochs):
    model.train()
    total_loss = 0
    total_samples = 0


    accuracy_metric.reset()
    auc_metric.reset()
    precision_metric.reset()
    recall_metric.reset()
    f1_metric.reset()

    for X_batch, y_batch in train_loader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device).float()

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * y_batch.size(0)
        total_samples += y_batch.size(0)


        accuracy_metric.update(outputs, y_batch)
        auc_metric.update(outputs, y_batch)
        precision_metric.update(outputs, y_batch)
        recall_metric.update(outputs, y_batch)
        f1_metric.update(outputs, y_batch)


    avg_loss = total_loss / total_samples
    acc = accuracy_metric.compute().item()
    auc = auc_metric.compute().item()
    prec = precision_metric.compute().item()
    rec = recall_metric.compute().item()
    f1 = f1_metric.compute().item()

    print(f"Epoch {epoch+1}")
    print(f"  Train Loss: {avg_loss:.4f}")
    print(f"  Accuracy:   {acc:.4f}")
    print(f"  AUC:        {auc:.4f}")
    print(f"  Precision:  {prec:.4f}")
    print(f"  Recall:     {rec:.4f}")
    print(f"  F1 Score:   {f1:.4f}")

from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, average_precision_score

def evaluate(model, loader, return_raw=False):
    model.eval()
    total_loss = 0
    total_samples = 0

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            total_loss += loss.item() * y_batch.size(0)
            total_samples += y_batch.size(0)

            all_preds.append(outputs.cpu())
            all_labels.append(y_batch.cpu())

    avg_loss = total_loss / total_samples
    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

    bin_preds = (all_preds > 0.24).astype(float)

    metrics = {
        "loss": avg_loss,
        "accuracy": accuracy_score(all_labels, bin_preds),
        "auc": roc_auc_score(all_labels, all_preds),
        "auprc": average_precision_score(all_labels, all_preds),
        "precision": precision_score(all_labels, bin_preds, zero_division=0),
        "recall": recall_score(all_labels, bin_preds, zero_division=0),
        "f1_score": f1_score(all_labels, bin_preds, zero_division=0)
    }

    if return_raw:
        return metrics, all_labels, all_preds
    else:
        return metrics

from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)
f1_scores = 2 * precision * recall / (precision + recall + 1e-8)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
plt.axvline(x=0.24, color='gray', linestyle='--', label='Chosen threshold (0.24)')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision, Recall, and F1 vs. Threshold")
plt.legend()
plt.grid(True)
plt.show()

val_metrics = evaluate(model, val_loader)
test_metrics = evaluate(model, test_loader)

print("Validation Metrics:")
for k, v in val_metrics.items():
    print(f"{k}: {v:.4f}")

print("\nTest Metrics:")
for k, v in test_metrics.items():
    print(f"{k}: {v:.4f}")

pip install captum # package for XAI

import torch
import numpy as np
import matplotlib.pyplot as plt
from captum.attr import IntegratedGradients


selected_feature_names = [
$
    'd2m', 'rh', 't2m', 'ssrd', 'wind_direction', 'vpd',


    'sp', 'smi', 'tp', 'wind_speed', 'ndvi', 'lai',


    'dem', 'slope', 'curvature', 'aspect', 'roads_distance', 'population'
]


selected_feature_indices = [feature_cols.index(f) for f in selected_feature_names]


# positive samples only (actual fires)
X_pos = X_test[y_test == 1]
if len(X_pos) == 0:
    raise ValueError("No positive samples in test set!")

X_pos_tensor = torch.tensor(X_pos, dtype=torch.float32).to(device)
baseline = torch.tensor(X_train.mean(axis=0), dtype=torch.float32).unsqueeze(0).to(device)

# --- Integrated Gradients Setup ---
ig = IntegratedGradients(model)
model.eval()

# --- Compute Attributions ---
attr_all = []
for i in range(len(X_pos_tensor)):
    input_tensor = X_pos_tensor[i:i+1].requires_grad_(True)
    attr, _ = ig.attribute(
        inputs=input_tensor,
        baselines=baseline,
        target=None,
        return_convergence_delta=True
    )
    attr_all.append(attr.squeeze(0).detach().cpu().numpy())

attr_all = np.stack(attr_all)


attr_all = attr_all[:, :, selected_feature_indices]


attr_mean = attr_all.mean(axis=0)
attr_std = attr_all.std(axis=0)


global_max = np.max(np.abs(attr_mean))
attr_mean_normalized = attr_mean / global_max


days_before = np.arange(30, 0, -1)


plt.figure(figsize=(15, 20))
for i, feature_name in enumerate(selected_feature_names):
    plt.subplot(6, 3, i+1)


    plt.plot(days_before, attr_mean_normalized[:, i], color='tab:blue', linewidth=2)


    plt.fill_between(
        days_before,
        attr_mean_normalized[:, i] - 0.1 * attr_std[:, i],
        attr_mean_normalized[:, i] + 0.1 * attr_std[:, i],
        color='tab:blue',
        alpha=0.2
    )

    plt.axvspan(1, 7, color='red', alpha=0.1)
    plt.title(feature_name)
    plt.gca().invert_xaxis()
    plt.grid(True, alpha=0.3)

    if i % 3 == 0:
        plt.ylabel('Normalized Attribution')
    if i >= 15:
        plt.xlabel('Days Before Ignition')

plt.tight_layout()

plt.savefig("/content/drive/MyDrive/Colab Notebooks/data/integrated_gradients_matrix.pdf", format='pdf', bbox_inches='tight')

plt.show()

from sklearn.metrics import average_precision_score
import numpy as np
import matplotlib.pyplot as plt

def permutation_importance_auprc(model, X_test, y_test, feature_cols):
    """
    Compute permutation importance using AUPRC

    Args:
        model: Your trained PyTorch model
        X_test: Test data (shape: n_samples × 30 × n_features)
        y_test: True labels (shape: n_samples)
        feature_cols: List of feature names (matches X_test's last dimension)

    Returns:
        Dictionary {feature_name: importance_score}
    """

    if isinstance(X_test, torch.Tensor):
        X_test = X_test.cpu().numpy()
    if isinstance(y_test, torch.Tensor):
        y_test = y_test.cpu().numpy()


    baseline_preds = predict_proba(model, X_test)
    baseline_score = average_precision_score(y_test, baseline_preds)

    importance = {}
    for feature_idx, feature_name in enumerate(feature_cols):

        X_perturbed = X_test.copy()


        X_perturbed[:, :, feature_idx] = np.random.permutation(
            X_perturbed[:, :, feature_idx]
        )


        perturbed_preds = predict_proba(model, X_perturbed)
        perturbed_score = average_precision_score(y_test, perturbed_preds)


        importance[feature_name] = baseline_score - perturbed_score

    return importance, baseline_score


def predict_proba(model, X):
    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    with torch.no_grad():
        outputs = model(X_tensor)
    return outputs.cpu().numpy()

feature_cols = [
    'aspect', 'curvature', 'd2m', 'dem', 'lai', 'ndvi', 'rh',
    'roads_distance', 'slope', 'smi', 'sp', 'ssrd', 't2m', 'tp',
    'wind_direction', 'wind_speed', 'population', 'vpd'
]

importance_dict, baseline_auprc = permutation_importance_auprc(
    model, X_test, y_test, feature_cols
)

sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
feature_names = [x[0] for x in sorted_features]
importance_scores = [x[1] for x in sorted_features]


plt.figure(figsize=(10, 6))
bars = plt.barh(feature_names, importance_scores, color='#1f77b4')


plt.xlabel('Decrease in AUPRC (Importance Score)', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.title(
    f'Permutation Feature Importance (AUPRC)\nBaseline AUPRC = {baseline_auprc:.3f}',
    fontsize=14, pad=20
)


for bar in bars:
    width = bar.get_width()
    plt.text(
        width + 0.001,
        bar.get_y() + bar.get_height()/2,
        f'{width:.3f}',
        ha='left', va='center', fontsize=10
    )


plt.gca().invert_yaxis()  # Most important at top
plt.grid(axis='x', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

def permutation_importance_auprc(model, X_test, y_test, feature_cols, n_repeats=5, random_state=None):
    """
    Compute permutation importance with multiple repetitions

    Args:
        model: Trained PyTorch model
        X_test: Test data (n_samples, seq_len, n_features)
        y_test: True labels (n_samples,)
        feature_cols: List of feature names
        n_repeats: Number of permutation repeats per feature
        random_state: For reproducibility

    Returns:
        Dictionary with mean and std of importance scores
    """
    if random_state is not None:
        np.random.seed(random_state)


    if isinstance(X_test, torch.Tensor):
        X_test = X_test.cpu().numpy()
    if isinstance(y_test, torch.Tensor):
        y_test = y_test.cpu().numpy()


    baseline_preds = predict_proba(model, X_test)
    baseline_score = average_precision_score(y_test, baseline_preds)


    importance = {name: [] for name in feature_cols}


    for _ in range(n_repeats):
        for feature_idx, feature_name in enumerate(feature_cols):

            X_perturbed = X_test.copy()


            X_perturbed[:, :, feature_idx] = np.random.permutation(
                X_perturbed[:, :, feature_idx]
            )


            perturbed_preds = predict_proba(model, X_perturbed)
            perturbed_score = average_precision_score(y_test, perturbed_preds)


            importance[feature_name].append(baseline_score - perturbed_score)


    result = {
        'mean': {k: np.mean(v) for k, v in importance.items()},
        'std': {k: np.std(v) for k, v in importance.items()},
        'baseline_auprc': baseline_score
    }

    return result


importance = permutation_importance_auprc(model, X_test, y_test, feature_cols, n_repeats=5)


sorted_features = sorted(importance['mean'].items(), key=lambda x: x[1], reverse=True)
names = [x[0] for x in sorted_features]
means = [x[1] for x in sorted_features]


plt.figure(figsize=(12, 8))
bars = plt.barh(names, means, color='firebrick')


plt.title(f"Permutation Feature Importance (AUPRC)\nBaseline Score = {importance['baseline_auprc']:.3f}", pad=20)
plt.xlabel("Mean Decrease in AUPRC")
plt.gca().invert_yaxis()


for bar, mean in zip(bars, means):
    plt.text(bar.get_width() + 0.005,
             bar.get_y() + bar.get_height()/2,
             f'{mean:.3f}',
             ha='left', va='center',
             fontsize=10)

plt.grid(axis='x', linestyle='--', alpha=0.3)
plt.tight_layout()

plt.savefig("/content/drive/MyDrive/Colab Notebooks/data/Permutation Feature Importance (AUPRC).pdf", format='pdf', bbox_inches='tight')

plt.show()
